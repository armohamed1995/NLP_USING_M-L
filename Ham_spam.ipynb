{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the SMSSpamCollection file \n",
    "\n",
    "rawdata = open('/home/armohammed/Desktop/Ex_Files_NLP_Python_ML_EssT/SMSSpamCollection').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " # replace /t with /n and perform split wherever there is /n  python will split the sentence \n",
    "\n",
    "parsedData = rawdata.replace('\\t','\\n')\n",
    "parsedData = parsedData.split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labellist = parsedData[0::2] # start from 0 column and select every second column. \n",
    "textlist = parsedData[1::2] # start from 0 column and select every first column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert into pandas data frame \n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "fullCorpus  = pd.DataFrame ({\n",
    "    \n",
    "    'label': labellist[:-1],\n",
    "    'body_list': textlist\n",
    "    \n",
    "})\n",
    "\n",
    "# save the file using  this line \n",
    "\n",
    "'''fullcorpus.to_csv()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data exploration \n",
    "# What is the shape of Dataset\n",
    "# what kind of Labels are there and how many are there \n",
    "# missing data \n",
    "\n",
    "'''load the file using \n",
    "\n",
    "data = pd.read_csv('your path to fullcorpus')\n",
    "\n",
    "'''\n",
    "print ('',data.shape, len(data[data['label']=='spam']), len(data[data['label']=='ham']) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "data['body_text_clean'] = data['body_text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "data.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def tokenize (text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens \n",
    "\n",
    "data ['body_text_tokenize'] = data['body_text_clean'].apply(lambda x:tokenize(x.lower()))\n",
    "\n",
    "# \"nlp\" = \"NLP\"  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#depending on computational power use stemming or lemmatizing \n",
    "\n",
    "# example of stemming and lematizing\n",
    "\n",
    "import nltk \n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer(\n",
    "    \n",
    "print('', ps.stem('goose')) # stemming\n",
    "print('', ps.stem('geese'))\n",
    "\n",
    "print('', wn.lemmatize('goose')) # lematizing\n",
    "print('', wn.lemmatize('geese'))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Note : Stemming or lemmatizing the data depends on your computational power, stemming uses algorithamic approach and simply chops off the word to its rootword, where as lemmatizing uses corpus dictonary to find suitable root word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming \n",
    "\n",
    "\n",
    "def stemming ( tokenize_text):\n",
    "    text  = [ ps.stem(word) for word in tokenize_text]\n",
    "    return text\n",
    "\n",
    "data['body_text_stemming '] = data['body_text_nostop'].apply(lambda x: stemming(x))\n",
    "\n",
    "# lemmatizing\n",
    "\n",
    "def lemmatizer(tokenized_text):\n",
    "    \n",
    "    lemmatize = [ wn.lemmatize(word)  for word in tokenize_text]    \n",
    "    return text\n",
    "\n",
    "data['body_text_lemmatize'] = data['body_text_nostop'].apply(lambda x: lemmatizer(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
